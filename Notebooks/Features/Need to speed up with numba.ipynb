{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "N_CPUS = 4\n",
    "def get_wap(bid_price, ask_price, bid_size, ask_size):\n",
    "    return (bid_price * ask_size + ask_price * bid_size) / (\n",
    "        bid_size + ask_size\n",
    "    )\n",
    "\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return ** 2))\n",
    "\n",
    "\n",
    "def get_realized_vol(book: pd.DataFrame):\n",
    "    \"\"\"Mutable method to get past realized volatility\"\"\"\n",
    "\n",
    "    book[\"wap1\"] = get_wap(\n",
    "        book.bid_price1, book.ask_price1, book.bid_size1, book.ask_size1\n",
    "    )\n",
    "\n",
    "    book[\"wap2\"] = get_wap(\n",
    "        book.bid_price2, book.ask_price2, book.bid_size2, book.ask_size2\n",
    "    )\n",
    "\n",
    "    a1 = (\n",
    "        book['bid_price1'] * book['ask_size1'] +\n",
    "        book['ask_price1'] * book['bid_size1']\n",
    "    )\n",
    "    a2 = (\n",
    "        book['bid_price2'] * book['ask_size2'] +\n",
    "        book['ask_price2'] * book['bid_size2']\n",
    "    )\n",
    "    b = (\n",
    "        book['bid_size1'] + book['ask_size1'] +\n",
    "        book['bid_size2']+ book['ask_size2']\n",
    "    )\n",
    "    book['wap3'] = (a1 + a2)/ b\n",
    "\n",
    "    book['wap4'] = (book['wap1'] + book['wap2']) / 2\n",
    "\n",
    "    book.loc[:, \"log_return1\"] = log_return(book[\"wap1\"])\n",
    "    book.loc[:, \"log_return2\"] = log_return(book[\"wap2\"])\n",
    "    book.loc[:, \"log_return3\"] = log_return(book[\"wap3\"])\n",
    "    book.loc[:, \"log_return4\"] = log_return(book[\"wap4\"])\n",
    "\n",
    "    # book = book[~book['log_return'].isnull()]\n",
    "    # to not remove rows both on train and test set\n",
    "    book = book.fillna(0)\n",
    "\n",
    "    book = book.merge(\n",
    "        book[[\"time_id\", \"log_return1\"]]\n",
    "        .rename({\"log_return1\": \"vol1\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    ).merge(\n",
    "        book[[\"time_id\", \"log_return2\"]]\n",
    "        .rename({\"log_return2\": \"vol2\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    ).merge(\n",
    "        book[[\"time_id\", \"log_return3\"]]\n",
    "        .rename({\"log_return3\": \"vol3\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    ).merge(\n",
    "        book[[\"time_id\", \"log_return4\"]]\n",
    "        .rename({\"log_return4\": \"vol4\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    )\n",
    "\n",
    "    book['wap_diff12'] = book['wap2'] - book['wap1']\n",
    "    book['wap_diff13'] = book['wap3'] - book['wap1']\n",
    "    book['wap_diff14'] = book['wap4'] - book['wap1']\n",
    "\n",
    "    book['vol_rate'] = (book['vol1'] / book['vol2']).fillna(0)\n",
    "    book['vol_diff'] = (book['vol1'] - book['vol2'])\n",
    "\n",
    "    return book\n",
    "\n",
    "\n",
    "# Price features (SLOWEST FUNCTION NEEDS TO IMPROVEEEEEEEEEEEEEEEEE)\n",
    "def get_stats(df, ind:str, column:str):\n",
    "    \"\"\"Get aggregated features from the column provided\"\"\"\n",
    "    stats = pd.merge(\n",
    "        df[[ind, column]].groupby(ind).mean()\n",
    "        .rename({column: f'{column}_mean'}, axis=1),\n",
    "        df[[ind, column]].groupby(ind).median()\n",
    "        .rename({column: f'{column}_median'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).count()\n",
    "        .rename({column: f'{column}_count'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).min()\n",
    "        .rename({column: f'{column}_min'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).max()\n",
    "        .rename({column: f'{column}_max'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).std()\n",
    "        .rename({column: f'{column}_std'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).quantile(0.25)\n",
    "        .rename({column: f'{column}_q25'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).quantile(0.75)\n",
    "        .rename({column: f'{column}_q75'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).nunique()\n",
    "        .rename({column: f'{column}_unique'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).mad()\n",
    "        .rename({column: f'{column}_mad'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).first()\n",
    "        .rename({column: f'{column}_first'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    ).merge(\n",
    "        df[[ind, column]].groupby(ind).last()\n",
    "        .rename({column: f'{column}_last'}, axis=1),\n",
    "        how='inner', left_index=True, right_index=True\n",
    "    )\n",
    "\n",
    "    stats[f'{column}_delta'] = (\n",
    "        stats[f'{column}_last'] - stats[f'{column}_first']\n",
    "    )\n",
    "    stats[f'{column}_delta_abs'] = (\n",
    "        stats[f'{column}_last'] - stats[f'{column}_first']\n",
    "    ).abs()\n",
    "    stats[f'{column}_unique_pct'] = (\n",
    "        stats[f'{column}_unique'] / stats[f'{column}_count']\n",
    "    )\n",
    "\n",
    "    del stats[f'{column}_count']\n",
    "    del stats[f'{column}_unique']\n",
    "    del stats[f'{column}_first']\n",
    "    del stats[f'{column}_last']\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_book_features(files):\n",
    "    pieces = []\n",
    "    c = 1\n",
    "    for f in tqdm(files):\n",
    "        book = pd.read_parquet(f)\n",
    "\n",
    "        book = get_realized_vol(book)\n",
    "\n",
    "        # price\n",
    "        book['spread1'] = book['ask_price1'] - book['bid_price1']\n",
    "        book['spread2'] = book['ask_price2'] - book['bid_price2']\n",
    "        book['ask_spread'] = book['ask_price2'] - book['ask_price1']\n",
    "        book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "        book['cross_spread1'] = book['ask_price1'] - book['bid_price1']\n",
    "        book['cross_spread2'] = book['ask_price2'] - book['bid_price2']\n",
    "        book['bas'] = (\n",
    "            book[['ask_price1', 'ask_price2']].min(axis = 1) /\n",
    "            book[['bid_price1', 'bid_price2']].max(axis = 1) -\n",
    "            1\n",
    "        )\n",
    "\n",
    "        # size\n",
    "        book['skew1'] = book['ask_size1'] - book['bid_size1']\n",
    "        book['skew2'] = book['ask_size2'] - book['bid_size2']\n",
    "        book['cross_skew1'] = book['ask_size1'] - book['bid_size2']\n",
    "        book['cross_skew2'] = book['ask_size2'] - book['bid_size1']\n",
    "        book['ask_sum'] = book['ask_size1'] - book['ask_size2']\n",
    "        book['bid_sum'] = book['bid_size1'] - book['bid_size2']\n",
    "        book['skew_whole'] = book['ask_sum'] - book['bid_sum']\n",
    "\n",
    "        # price - size combinations\n",
    "        book['bid_volume1'] = book['bid_price1'] * book['bid_size1']\n",
    "        book['bid_volume2'] = book['bid_price2'] * book['bid_size2']\n",
    "        book['ask_volume1'] = book['ask_price1'] * book['ask_size1']\n",
    "        book['ask_volume2'] = book['ask_price2'] * book['ask_size2']\n",
    "\n",
    "        # sum of volumes\n",
    "        book_sums = (\n",
    "            book[['time_id',\n",
    "                'bid_volume1', 'bid_volume2',\n",
    "                'ask_volume1','ask_volume2']]\n",
    "            .groupby('time_id').sum()\n",
    "            .rename({\n",
    "                'bid_volume1': 'bid_volume1_sum',\n",
    "                'bid_volume2': 'bid_volume2_sum',\n",
    "                'ask_volume1': 'ask_volume1_sum',\n",
    "                'ask_volume2': 'ask_volume2_sum'\n",
    "            }, axis=1)\n",
    "        )\n",
    "        features_to_get_stats = [\n",
    "            'wap1', 'wap2', 'wap3', 'wap4',\n",
    "            'log_return1', 'log_return2', 'log_return3', 'log_return4',\n",
    "            'vol1', 'vol2', 'vol3', 'vol4', 'vol_rate', 'vol_diff',\n",
    "            'wap_diff12', 'wap_diff13', 'wap_diff14',\n",
    "            'bid_price1', 'bid_price2', 'ask_price1', 'ask_price2',\n",
    "            'spread1', 'spread2', 'ask_spread', 'bid_spread', 'cross_spread1', 'cross_spread2', 'bas',\n",
    "            'bid_size1', 'bid_size2', 'ask_size1', 'ask_size2',\n",
    "            'skew1', 'skew2', 'cross_skew1', 'cross_skew2', 'ask_sum', 'bid_sum', 'skew_whole',\n",
    "            'bid_volume1', 'bid_volume2', 'ask_volume1', 'ask_volume2',\n",
    "        ]\n",
    "        stats_df = get_stats(book,'time_id','seconds_in_bucket')\n",
    "        for i in features_to_get_stats:\n",
    "            stats_df = pd.concat([stats_df,get_stats(book,'time_id',i)])\n",
    "            print(c)\n",
    "        # do not merge with not aggregated book\n",
    "        dfs = [\n",
    "            book_sums,\n",
    "            stats_df\n",
    "        ]\n",
    "        df_stats = reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left, right,\n",
    "                how='inner', left_index=True, right_index=True\n",
    "            ),\n",
    "            dfs\n",
    "        )\n",
    "        df_stats[\"stock_id\"] = int(f.split(\"=\")[-1])\n",
    "        df_stats['time_id'] = df_stats.index\n",
    "        df_stats = df_stats.fillna(method = 'backfill')\n",
    "        df_stats = df_stats.fillna(method = 'ffill')\n",
    "#         df_stats.drop('time_id',axis=1,inplace=True)\n",
    "        df_stats = df_stats.iloc[0,:].reset_index()\n",
    "        dataset_new = pd.DataFrame(data=[np.array(list(df_stats.iloc[:,1]))],columns = list(df_stats.iloc[:,0]))\n",
    "        if c == 1:\n",
    "            data = dataset_new.copy()\n",
    "        else:\n",
    "            data = pd.concat([data,dataset_new])\n",
    "        c+=1\n",
    "        print(c)\n",
    "        if c==5:\n",
    "            return data , list(data.keys())\n",
    "\n",
    "\n",
    "def get_trade_features(files):\n",
    "    \"\"\"Getting features from trading history\"\"\"\n",
    "\n",
    "    pieces = []\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        trades = pd.read_parquet(f)\n",
    "\n",
    "        trades['trade_volume'] = trades['price'] * trades['size']\n",
    "        trades['trade_size_per_order'] = (\n",
    "            trades['size'] / trades['order_count']\n",
    "        )\n",
    "        trades['trade_volume_per_order_mean'] = (\n",
    "            trades['trade_volume'] / trades['trade_size_per_order']\n",
    "        )\n",
    "\n",
    "        trades = trades.rename(\n",
    "            {\n",
    "                'price': 'trade_price',\n",
    "                'order_count': 'trade_order_count',\n",
    "                'seconds_in_bucket': 'trade_seconds_in_bucket'\n",
    "            },\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # sum of volumes, orders and sizes\n",
    "        trades_sums = (\n",
    "            trades[['time_id',\n",
    "                'size', 'trade_order_count', 'trade_volume'\n",
    "            ]]\n",
    "            .groupby('time_id').sum()\n",
    "            .rename({\n",
    "                'size': 'size_sum',\n",
    "                'trade_order_count': 'trade_order_count_sum',\n",
    "                'trade_volume': 'trade_volume_sum'\n",
    "            }, axis=1)\n",
    "        )\n",
    "\n",
    "        # volatility of trades\n",
    "        trades.loc[:, \"trade_log_return\"] = log_return(trades[\"trade_price\"])\n",
    "\n",
    "        trades_vol = (\n",
    "            trades[[\"time_id\", \"trade_log_return\"]]\n",
    "            .rename({\"trade_log_return\": \"trade_vol\"}, axis=1)\n",
    "            .groupby(\"time_id\")\n",
    "            .agg(realized_volatility)\n",
    "        )\n",
    "\n",
    "        features_to_get_stats = [  \n",
    "            'trade_price', 'size', 'trade_order_count',\n",
    "            'trade_volume', 'trade_size_per_order', 'trade_volume_per_order_mean',\n",
    "            'trade_log_return'\n",
    "        ]\n",
    "        stats_df = get_stats(trades,'time_id','trade_seconds_in_bucket')# where operations are located in the bucket\n",
    "        for i in features_to_get_stats:\n",
    "            stats_df = pd.concat([stats_df,get_stats(trades,'time_id',i)])\n",
    "        # do not merge with not aggregated trades\n",
    "        dfs = [\n",
    "            trades_sums,\n",
    "            trades_vol,\n",
    "            stats_df\n",
    "        ]\n",
    "\n",
    "        dfs.append(\n",
    "            trades[['time_id', 'trade_seconds_in_bucket']]\n",
    "            .groupby('time_id').count()\n",
    "            .rename({'trade_seconds_in_bucket': 'n_trades'}, axis=1)\n",
    "        )\n",
    "\n",
    "        df_stats = reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left, right, how='inner', left_index=True, right_index=True\n",
    "            ),\n",
    "            dfs\n",
    "        )\n",
    "\n",
    "\n",
    "        df_stats[\"stock_id\"] = int(f.split(\"=\")[-1])\n",
    "#         pieces.append(df_stats)\n",
    "#     dataset_new = pd.concat(pieces).reset_index()\n",
    "    \n",
    "#     features = list(dataset_new.keys())\n",
    "        df_stats['time_id'] = df_stats.index\n",
    "        df_stats = df_stats.fillna(method = 'backfill')\n",
    "        df_stats = df_stats.fillna(method = 'ffill')\n",
    "#         df_stats.drop('time_id',axis=1,inplace=True)\n",
    "        df_stats = df_stats.iloc[0,:].reset_index()\n",
    "        dataset_new = pd.DataFrame(data=[np.array(list(df_stats.iloc[:,1]))],columns = list(df_stats.iloc[:,0]))\n",
    "    return dataset_new, list(dataset_new.keys())\n",
    "\n",
    "\n",
    "def mean_encoding(\n",
    "    dataset: pd.DataFrame,\n",
    "    means: dict = None,\n",
    "    stds: dict = None,\n",
    "    medians: dict = None\n",
    "):\n",
    "    \"\"\"Dataset with stock_id and target columns\"\"\"\n",
    "    if means is None:\n",
    "        means = (\n",
    "            dataset[[\"stock_id\", \"target\"]]\n",
    "            .groupby(\"stock_id\").mean()\n",
    "        )\n",
    "        means = means['target'].to_dict()\n",
    "\n",
    "        stds = (\n",
    "            dataset[[\"stock_id\", \"target\"]]\n",
    "            .groupby(\"stock_id\").std()\n",
    "        )\n",
    "        stds = stds['target'].to_dict()\n",
    "\n",
    "        medians = (\n",
    "            dataset[[\"stock_id\", \"target\"]]\n",
    "            .groupby(\"stock_id\").median()\n",
    "        )\n",
    "        medians = medians['target'].to_dict()\n",
    "\n",
    "    dataset[\"stock_id_mean\"] = dataset[\"stock_id\"].apply(lambda x: means[x])\n",
    "    dataset[\"stock_id_std\"] = dataset[\"stock_id\"].apply(lambda x: stds[x])\n",
    "    dataset[\"stock_id_median\"] = dataset[\"stock_id\"].apply(lambda x: medians[x])\n",
    "    del dataset['stock_id']\n",
    "\n",
    "    return dataset, (means, stds, medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"D:/Kaggle/optiver-realized-volatility-prediction/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/112 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\n",
    "    \"D:/Kaggle/optiver-realized-volatility-prediction/book_train.parquet/*\"\n",
    ")\n",
    "books, features_book = get_book_features(files)\n",
    "\n",
    "dataset_new = pd.merge(\n",
    "    books,\n",
    "    dataset[[\"time_id\", \"stock_id\",'row_id']],\n",
    "    how=\"inner\",\n",
    "    on=[\"time_id\", \"stock_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "files_trade = glob.glob(\n",
    "    \"D:/Kaggle/optiver-realized-volatility-prediction/trade_train.parquet/*\"\n",
    ")\n",
    "trade_stats, features_trade = get_trade_features(files_trade)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataset\n",
    "dataset_new = pd.merge(\n",
    "    dataset_new,\n",
    "    trade_stats,\n",
    "    how='inner', on=['time_id', 'stock_id'],\n",
    "    left_index=False, right_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_trade = glob.glob(\n",
    "    \"../input/optiver-realized-volatility-prediction/trade_test.parquet/*\"\n",
    ")\n",
    "trade_stats, features_trade = get_trade_features(files_trade)\n",
    "\n",
    "# merging dataset\n",
    "dataset_new = pd.merge(\n",
    "    dataset_new,\n",
    "    trade_stats,\n",
    "    how='inner', on=['time_id', 'stock_id'],\n",
    "    left_index=False, right_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_new\n",
    "train_df = pd.read_csv('D:/Kaggle/Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(train_df.drop('target',axis=1).columns)\n",
    "a.append('row_id')\n",
    "test_df = test_df[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_x , train_y = train_df.iloc[:,:-1],train_df.iloc[:,-1]\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(147,)))\n",
    "model.add(tf.keras.layers.Dense(10,activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(5,activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='tanh'))\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.MeanAbsolutePercentageError(),metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x,train_y,batch_size=128,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_df.iloc[:,:-1])\n",
    "test_df['target'] = pred\n",
    "test_df[['row_id','target']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
