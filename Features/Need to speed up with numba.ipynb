{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "N_CPUS = 4\n",
    "def get_wap(bid_price, ask_price, bid_size, ask_size):\n",
    "    return (bid_price * ask_size + ask_price * bid_size) / (\n",
    "        bid_size + ask_size\n",
    "    )\n",
    "\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return ** 2))\n",
    "\n",
    "\n",
    "def get_realized_vol(book: pd.DataFrame):\n",
    "    \"\"\"Mutable method to get past realized volatility\"\"\"\n",
    "\n",
    "    book[\"wap1\"] = get_wap(\n",
    "        book.bid_price1, book.ask_price1, book.bid_size1, book.ask_size1\n",
    "    )\n",
    "\n",
    "    book[\"wap2\"] = get_wap(\n",
    "        book.bid_price2, book.ask_price2, book.bid_size2, book.ask_size2\n",
    "    )\n",
    "\n",
    "    a1 = (\n",
    "        book['bid_price1'] * book['ask_size1'] +\n",
    "        book['ask_price1'] * book['bid_size1']\n",
    "    )\n",
    "    a2 = (\n",
    "        book['bid_price2'] * book['ask_size2'] +\n",
    "        book['ask_price2'] * book['bid_size2']\n",
    "    )\n",
    "    b = (\n",
    "        book['bid_size1'] + book['ask_size1'] +\n",
    "        book['bid_size2']+ book['ask_size2']\n",
    "    )\n",
    "    book['wap3'] = (a1 + a2)/ b\n",
    "\n",
    "    book['wap4'] = (book['wap1'] + book['wap2']) / 2\n",
    "\n",
    "    book.loc[:, \"log_return1\"] = log_return(book[\"wap1\"])\n",
    "    book.loc[:, \"log_return2\"] = log_return(book[\"wap2\"])\n",
    "    book.loc[:, \"log_return3\"] = log_return(book[\"wap3\"])\n",
    "    book.loc[:, \"log_return4\"] = log_return(book[\"wap4\"])\n",
    "\n",
    "    # book = book[~book['log_return'].isnull()]\n",
    "    # to not remove rows both on train and test set\n",
    "    book = book.fillna(0)\n",
    "\n",
    "    book = book.merge(\n",
    "        book[[\"time_id\", \"log_return1\"]]\n",
    "        .rename({\"log_return1\": \"vol1\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    ).merge(\n",
    "        book[[\"time_id\", \"log_return2\"]]\n",
    "        .rename({\"log_return2\": \"vol2\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    ).merge(\n",
    "        book[[\"time_id\", \"log_return3\"]]\n",
    "        .rename({\"log_return3\": \"vol3\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    ).merge(\n",
    "        book[[\"time_id\", \"log_return4\"]]\n",
    "        .rename({\"log_return4\": \"vol4\"}, axis=1)\n",
    "        .groupby(\"time_id\")\n",
    "        .agg(realized_volatility),\n",
    "        how=\"inner\",\n",
    "        on=\"time_id\",\n",
    "    )\n",
    "\n",
    "    book['wap_diff12'] = book['wap2'] - book['wap1']\n",
    "    book['wap_diff13'] = book['wap3'] - book['wap1']\n",
    "    book['wap_diff14'] = book['wap4'] - book['wap1']\n",
    "\n",
    "    book['vol_rate'] = (book['vol1'] / book['vol2']).fillna(0)\n",
    "    book['vol_diff'] = (book['vol1'] - book['vol2'])\n",
    "\n",
    "    return book\n",
    "\n",
    "\n",
    "\n",
    "# Price features (SLOWEST FUNCTION NEEDS TO IMPROVEEEEEEEEEEEEEEEEE)\n",
    "def get_stats(df, ind:str, column:str):\n",
    "    \"\"\"Get aggregated features from the column provided\"\"\"\n",
    "    stats = pd.concat([df[[ind, column]].groupby(ind).mean().rename({column: f'{column}_mean'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).median()\n",
    "    .rename({column: f'{column}_median'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).count()\n",
    "    .rename({column: f'{column}_count'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).min()\n",
    "    .rename({column: f'{column}_min'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).max()\n",
    "    .rename({column: f'{column}_max'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).std()\n",
    "    .rename({column: f'{column}_std'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).quantile(0.25)\n",
    "    .rename({column: f'{column}_q25'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).quantile(0.75)\n",
    "    .rename({column: f'{column}_q75'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).nunique()\n",
    "    .rename({column: f'{column}_unique'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).mad()\n",
    "    .rename({column: f'{column}_mad'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).first()\n",
    "    .rename({column: f'{column}_first'}, axis=1),\n",
    "    df[[ind, column]].groupby(ind).last()\n",
    "    .rename({column: f'{column}_last'}, axis=1)],\n",
    "    axis=1)\n",
    "\n",
    "    stats[f'{column}_delta'] = (\n",
    "        stats[f'{column}_last'] - stats[f'{column}_first']\n",
    "    )\n",
    "    stats[f'{column}_delta_abs'] = (\n",
    "        stats[f'{column}_last'] - stats[f'{column}_first']\n",
    "    ).abs()\n",
    "    stats[f'{column}_unique_pct'] = (\n",
    "        stats[f'{column}_unique'] / stats[f'{column}_count']\n",
    "    )\n",
    "\n",
    "    del stats[f'{column}_count']\n",
    "    del stats[f'{column}_unique']\n",
    "    del stats[f'{column}_first']\n",
    "    del stats[f'{column}_last']\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_book_features(files):\n",
    "    pieces = []\n",
    "    c = 1\n",
    "    for f in tqdm(files):\n",
    "        book = pd.read_parquet(f)\n",
    "\n",
    "        book = get_realized_vol(book)\n",
    "\n",
    "        # price\n",
    "        book['spread1'] = book['ask_price1'] - book['bid_price1']\n",
    "        book['spread2'] = book['ask_price2'] - book['bid_price2']\n",
    "        book['ask_spread'] = book['ask_price2'] - book['ask_price1']\n",
    "        book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "        book['cross_spread1'] = book['ask_price1'] - book['bid_price1']\n",
    "        book['cross_spread2'] = book['ask_price2'] - book['bid_price2']\n",
    "        book['bas'] = (\n",
    "            book[['ask_price1', 'ask_price2']].min(axis = 1) /\n",
    "            book[['bid_price1', 'bid_price2']].max(axis = 1) -\n",
    "            1\n",
    "        )\n",
    "\n",
    "        # size\n",
    "        book['skew1'] = book['ask_size1'] - book['bid_size1']\n",
    "        book['skew2'] = book['ask_size2'] - book['bid_size2']\n",
    "        book['cross_skew1'] = book['ask_size1'] - book['bid_size2']\n",
    "        book['cross_skew2'] = book['ask_size2'] - book['bid_size1']\n",
    "        book['ask_sum'] = book['ask_size1'] - book['ask_size2']\n",
    "        book['bid_sum'] = book['bid_size1'] - book['bid_size2']\n",
    "        book['skew_whole'] = book['ask_sum'] - book['bid_sum']\n",
    "\n",
    "        # price - size combinations\n",
    "        book['bid_volume1'] = book['bid_price1'] * book['bid_size1']\n",
    "        book['bid_volume2'] = book['bid_price2'] * book['bid_size2']\n",
    "        book['ask_volume1'] = book['ask_price1'] * book['ask_size1']\n",
    "        book['ask_volume2'] = book['ask_price2'] * book['ask_size2']\n",
    "\n",
    "        # sum of volumes\n",
    "        book_sums = (\n",
    "            book[['time_id',\n",
    "                'bid_volume1', 'bid_volume2',\n",
    "                'ask_volume1','ask_volume2']]\n",
    "            .groupby('time_id').sum()\n",
    "            .rename({\n",
    "                'bid_volume1': 'bid_volume1_sum',\n",
    "                'bid_volume2': 'bid_volume2_sum',\n",
    "                'ask_volume1': 'ask_volume1_sum',\n",
    "                'ask_volume2': 'ask_volume2_sum'\n",
    "            }, axis=1)\n",
    "        )\n",
    "        features_to_get_stats = [\n",
    "            'wap2', 'wap3', 'wap4',\n",
    "            'log_return1', 'log_return2', 'log_return3', 'log_return4',\n",
    "            'vol1', 'vol2', 'vol3', 'vol4', 'vol_diff',\n",
    "            'wap_diff12', 'wap_diff13', 'wap_diff14',\n",
    "            'bid_price1', 'bid_price2', 'ask_price1', 'ask_price2',\n",
    "            'spread1', 'spread2', 'ask_spread', 'bid_spread', 'cross_spread1', 'cross_spread2', 'bas'\n",
    "        ]\n",
    "        stats_df = get_stats(book,'time_id','wap1')\n",
    "        for i in features_to_get_stats:\n",
    "            stats_df = pd.concat([stats_df,get_stats(book,'time_id',i)])\n",
    "            print(c)\n",
    "        # do not merge with not aggregated book\n",
    "        dfs = [\n",
    "            book_sums,\n",
    "            stats_df\n",
    "        ]\n",
    "        df_stats = reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left, right,\n",
    "                how='inner', left_index=True, right_index=True\n",
    "            ),\n",
    "            dfs\n",
    "        )\n",
    "        df_stats[\"stock_id\"] = int(f.split(\"=\")[-1])\n",
    "        df_stats['time_id'] = df_stats.index\n",
    "        df_stats = df_stats.fillna(method = 'backfill')\n",
    "        df_stats = df_stats.fillna(method = 'ffill')\n",
    "#         df_stats.drop('time_id',axis=1,inplace=True)\n",
    "        df_stats = df_stats.iloc[0,:].reset_index()\n",
    "        dataset_new = pd.DataFrame(data=[np.array(list(df_stats.iloc[:,1]))],columns = list(df_stats.iloc[:,0]))\n",
    "        if c == 1:\n",
    "            data = dataset_new.copy()\n",
    "            \n",
    "        else:\n",
    "            data = pd.concat([data,dataset_new])\n",
    "        c+=1\n",
    "        print(c)\n",
    "        if c >= 4:\n",
    "            return data , list(data.keys())\n",
    "    return data , list(data.keys())\n",
    "\n",
    "\n",
    "def get_trade_features(files):\n",
    "    \"\"\"Getting features from trading history\"\"\"\n",
    "    c = 1\n",
    "    pieces = []\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        trades = pd.read_parquet(f)\n",
    "\n",
    "        trades['trade_volume'] = trades['price'] * trades['size']\n",
    "        trades['trade_size_per_order'] = (\n",
    "            trades['size'] / trades['order_count']\n",
    "        )\n",
    "        trades['trade_volume_per_order_mean'] = (\n",
    "            trades['trade_volume'] / trades['trade_size_per_order']\n",
    "        )\n",
    "\n",
    "        trades = trades.rename(\n",
    "            {\n",
    "                'price': 'trade_price',\n",
    "                'order_count': 'trade_order_count',\n",
    "                'seconds_in_bucket': 'trade_seconds_in_bucket'\n",
    "            },\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # sum of volumes, orders and sizes\n",
    "        trades_sums = (\n",
    "            trades[['time_id',\n",
    "                'size', 'trade_order_count', 'trade_volume'\n",
    "            ]]\n",
    "            .groupby('time_id').sum()\n",
    "            .rename({\n",
    "                'size': 'size_sum',\n",
    "                'trade_order_count': 'trade_order_count_sum',\n",
    "                'trade_volume': 'trade_volume_sum'\n",
    "            }, axis=1)\n",
    "        )\n",
    "\n",
    "        # volatility of trades\n",
    "        trades.loc[:, \"trade_log_return\"] = log_return(trades[\"trade_price\"])\n",
    "\n",
    "        trades_vol = (\n",
    "            trades[[\"time_id\", \"trade_log_return\"]]\n",
    "            .rename({\"trade_log_return\": \"trade_vol\"}, axis=1)\n",
    "            .groupby(\"time_id\")\n",
    "            .agg(realized_volatility)\n",
    "        )\n",
    "\n",
    "        features_to_get_stats = ['trade_log_return']\n",
    "        stats_df = get_stats(trades,'time_id','trade_price')# where operations are located in the bucket\n",
    "        for i in features_to_get_stats:\n",
    "            print(c)\n",
    "            stats_df = pd.concat([stats_df,get_stats(trades,'time_id',i)])\n",
    "        # do not merge with not aggregated trades\n",
    "        dfs = [\n",
    "            trades_sums,\n",
    "            trades_vol,\n",
    "            stats_df\n",
    "        ]\n",
    "\n",
    "        dfs.append(\n",
    "            trades[['time_id', 'trade_seconds_in_bucket']]\n",
    "            .groupby('time_id').count()\n",
    "            .rename({'trade_seconds_in_bucket': 'n_trades'}, axis=1)\n",
    "        )\n",
    "\n",
    "        df_stats = reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left, right, how='inner', left_index=True, right_index=True\n",
    "            ),\n",
    "            dfs\n",
    "        )\n",
    "\n",
    "\n",
    "        df_stats[\"stock_id\"] = int(f.split(\"=\")[-1])\n",
    "#         pieces.append(df_stats)\n",
    "#     dataset_new = pd.concat(pieces).reset_index()\n",
    "    \n",
    "#     features = list(dataset_new.keys())\n",
    "        df_stats['time_id'] = df_stats.index\n",
    "        df_stats = df_stats.fillna(method = 'backfill')\n",
    "        df_stats = df_stats.fillna(method = 'ffill')\n",
    "#         df_stats.drop('time_id',axis=1,inplace=True)\n",
    "        df_stats = df_stats.iloc[0,:].reset_index()\n",
    "        dataset_new = pd.DataFrame(data=[np.array(list(df_stats.iloc[:,1]))],columns = list(df_stats.iloc[:,0]))\n",
    "        if c == 1:\n",
    "            data = dataset_new.copy()\n",
    "        else:\n",
    "            data = pd.concat([dataset_new,data])\n",
    "        c+= 1\n",
    "        if c >= 3:\n",
    "            return data, list(data.keys())\n",
    "    return data, list(data.keys())\n",
    "\n",
    "\n",
    "def mean_encoding(\n",
    "    dataset: pd.DataFrame,\n",
    "    means: dict = None,\n",
    "    stds: dict = None,\n",
    "    medians: dict = None\n",
    "):\n",
    "    \"\"\"Dataset with stock_id and target columns\"\"\"\n",
    "    if means is None:\n",
    "        means = (\n",
    "            dataset[[\"stock_id\", \"target\"]]\n",
    "            .groupby(\"stock_id\").mean()\n",
    "        )\n",
    "        means = means['target'].to_dict()\n",
    "\n",
    "        stds = (\n",
    "            dataset[[\"stock_id\", \"target\"]]\n",
    "            .groupby(\"stock_id\").std()\n",
    "        )\n",
    "        stds = stds['target'].to_dict()\n",
    "\n",
    "        medians = (\n",
    "            dataset[[\"stock_id\", \"target\"]]\n",
    "            .groupby(\"stock_id\").median()\n",
    "        )\n",
    "        medians = medians['target'].to_dict()\n",
    "\n",
    "    dataset[\"stock_id_mean\"] = dataset[\"stock_id\"].apply(lambda x: means[x])\n",
    "    dataset[\"stock_id_std\"] = dataset[\"stock_id\"].apply(lambda x: stds[x])\n",
    "    dataset[\"stock_id_median\"] = dataset[\"stock_id\"].apply(lambda x: medians[x])\n",
    "    del dataset['stock_id']\n",
    "\n",
    "    return dataset, (means, stds, medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target     row_id\n",
       "0              0        5  0.004136        0-5\n",
       "1              0       11  0.001445       0-11\n",
       "2              0       16  0.002168       0-16\n",
       "3              0       31  0.002195       0-31\n",
       "4              0       62  0.001747       0-62\n",
       "...          ...      ...       ...        ...\n",
       "428927       126    32751  0.003461  126-32751\n",
       "428928       126    32753  0.003113  126-32753\n",
       "428929       126    32758  0.004070  126-32758\n",
       "428930       126    32763  0.003357  126-32763\n",
       "428931       126    32767  0.002090  126-32767\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"D:/Kaggle/optiver-realized-volatility-prediction/train.csv\")\n",
    "dataset['row_id'] = dataset['stock_id'].astype(str) + '-' + dataset['time_id'].astype(str)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\n",
    "    \"D:/Kaggle/optiver-realized-volatility-prediction/book_train.parquet/*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/112 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▋                                                                              | 1/112 [02:06<3:54:38, 126.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|█▍                                                                             | 2/112 [04:34<4:04:00, 133.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "books, features_book = get_book_features(files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = pd.merge(\n",
    "    books,\n",
    "    dataset[[\"time_id\", \"stock_id\",'row_id']],\n",
    "    how=\"inner\",\n",
    "    on=[\"time_id\", \"stock_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_volume1_sum</th>\n",
       "      <th>bid_volume2_sum</th>\n",
       "      <th>ask_volume1_sum</th>\n",
       "      <th>ask_volume2_sum</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_median</th>\n",
       "      <th>wap1_min</th>\n",
       "      <th>wap1_max</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap1_q25</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_log_return_min</th>\n",
       "      <th>trade_log_return_max</th>\n",
       "      <th>trade_log_return_std</th>\n",
       "      <th>trade_log_return_q25</th>\n",
       "      <th>trade_log_return_q75</th>\n",
       "      <th>trade_log_return_mad</th>\n",
       "      <th>trade_log_return_delta</th>\n",
       "      <th>trade_log_return_delta_abs</th>\n",
       "      <th>trade_log_return_unique_pct</th>\n",
       "      <th>n_trades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23716.371095</td>\n",
       "      <td>24506.294559</td>\n",
       "      <td>22618.740013</td>\n",
       "      <td>27228.657619</td>\n",
       "      <td>1.003725</td>\n",
       "      <td>1.003923</td>\n",
       "      <td>1.001434</td>\n",
       "      <td>1.004920</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>1.003558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35867.289062</td>\n",
       "      <td>31272.945312</td>\n",
       "      <td>64039.640625</td>\n",
       "      <td>34861.621094</td>\n",
       "      <td>1.003585</td>\n",
       "      <td>1.003407</td>\n",
       "      <td>1.000780</td>\n",
       "      <td>1.006438</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>1.002761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.978495</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 397 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bid_volume1_sum  bid_volume2_sum  ask_volume1_sum  ask_volume2_sum  \\\n",
       "0     23716.371095     24506.294559     22618.740013     27228.657619   \n",
       "1     35867.289062     31272.945312     64039.640625     34861.621094   \n",
       "\n",
       "   wap1_mean  wap1_median  wap1_min  wap1_max  wap1_std  wap1_q25  ...  \\\n",
       "0   1.003725     1.003923  1.001434  1.004920  0.000693  1.003558  ...   \n",
       "1   1.003585     1.003407  1.000780  1.006438  0.001174  1.002761  ...   \n",
       "\n",
       "   trade_log_return_min  trade_log_return_max  trade_log_return_std  \\\n",
       "0             -0.000721              0.000610              0.000324   \n",
       "1             -0.000754              0.000691              0.000322   \n",
       "\n",
       "   trade_log_return_q25  trade_log_return_q75  trade_log_return_mad  \\\n",
       "0             -0.000222              0.000295              0.000271   \n",
       "1             -0.000244              0.000326              0.000274   \n",
       "\n",
       "   trade_log_return_delta  trade_log_return_delta_abs  \\\n",
       "0               -0.000308                    0.000308   \n",
       "1               -0.000510                    0.000510   \n",
       "\n",
       "   trade_log_return_unique_pct  n_trades  \n",
       "0                     1.000000      40.0  \n",
       "1                     0.978495      94.0  \n",
       "\n",
       "[2 rows x 397 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/112 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▋                                                                                 | 1/112 [00:08<15:22,  8.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "files_trade = glob.glob(\n",
    "    \"D:/Kaggle/optiver-realized-volatility-prediction/trade_train.parquet/*\"\n",
    ")\n",
    "trade_stats, features_trade = get_trade_features(files_trade)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataset\n",
    "dataset_new = pd.merge(\n",
    "    dataset_new,\n",
    "    trade_stats,\n",
    "    how='inner', on=['time_id', 'stock_id'],\n",
    "    left_index=False, right_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_new.copy()\n",
    "train_df = pd.read_csv('D:/Kaggle/optiver-realized-volatility-prediction/Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wap1_std',\n",
       " 'wap1_mad',\n",
       " 'wap1_delta_abs',\n",
       " 'wap2_std',\n",
       " 'wap2_mad',\n",
       " 'wap2_delta_abs',\n",
       " 'wap3_std',\n",
       " 'wap3_mad',\n",
       " 'wap3_delta_abs',\n",
       " 'wap4_std',\n",
       " 'wap4_mad',\n",
       " 'wap4_delta_abs',\n",
       " 'log_return1_std',\n",
       " 'log_return1_q25',\n",
       " 'log_return1_q75',\n",
       " 'log_return1_mad',\n",
       " 'log_return2_std',\n",
       " 'log_return2_q25',\n",
       " 'log_return2_q75',\n",
       " 'log_return2_mad',\n",
       " 'log_return3_std',\n",
       " 'log_return3_q25',\n",
       " 'log_return3_q75',\n",
       " 'log_return3_mad',\n",
       " 'log_return4_std',\n",
       " 'log_return4_q25',\n",
       " 'log_return4_q75',\n",
       " 'log_return4_mad',\n",
       " 'vol1_mean',\n",
       " 'vol1_median',\n",
       " 'vol1_min',\n",
       " 'vol1_max',\n",
       " 'vol1_q25',\n",
       " 'vol1_q75',\n",
       " 'vol2_mean',\n",
       " 'vol2_median',\n",
       " 'vol2_min',\n",
       " 'vol2_max',\n",
       " 'vol2_q25',\n",
       " 'vol2_q75',\n",
       " 'vol2_mad',\n",
       " 'vol3_mean',\n",
       " 'vol3_median',\n",
       " 'vol3_min',\n",
       " 'vol3_max',\n",
       " 'vol3_q25',\n",
       " 'vol3_q75',\n",
       " 'vol4_mean',\n",
       " 'vol4_median',\n",
       " 'vol4_min',\n",
       " 'vol4_max',\n",
       " 'vol4_q25',\n",
       " 'vol4_q75',\n",
       " 'vol_diff_mean',\n",
       " 'vol_diff_median',\n",
       " 'vol_diff_min',\n",
       " 'vol_diff_max',\n",
       " 'vol_diff_q25',\n",
       " 'vol_diff_q75',\n",
       " 'vol_diff_mad',\n",
       " 'wap_diff12_min',\n",
       " 'wap_diff12_max',\n",
       " 'wap_diff12_std',\n",
       " 'wap_diff12_q25',\n",
       " 'wap_diff12_q75',\n",
       " 'wap_diff12_mad',\n",
       " 'wap_diff13_min',\n",
       " 'wap_diff13_max',\n",
       " 'wap_diff13_std',\n",
       " 'wap_diff13_q25',\n",
       " 'wap_diff13_q75',\n",
       " 'wap_diff13_mad',\n",
       " 'wap_diff14_min',\n",
       " 'wap_diff14_max',\n",
       " 'wap_diff14_std',\n",
       " 'wap_diff14_q25',\n",
       " 'wap_diff14_q75',\n",
       " 'wap_diff14_mad',\n",
       " 'bid_price1_std',\n",
       " 'bid_price1_mad',\n",
       " 'bid_price1_delta_abs',\n",
       " 'bid_price1_unique_pct',\n",
       " 'bid_price2_std',\n",
       " 'bid_price2_mad',\n",
       " 'bid_price2_delta_abs',\n",
       " 'bid_price2_unique_pct',\n",
       " 'ask_price1_std',\n",
       " 'ask_price1_mad',\n",
       " 'ask_price1_delta_abs',\n",
       " 'ask_price1_unique_pct',\n",
       " 'ask_price2_std',\n",
       " 'ask_price2_mad',\n",
       " 'ask_price2_delta_abs',\n",
       " 'ask_price2_unique_pct',\n",
       " 'spread1_mean',\n",
       " 'spread1_median',\n",
       " 'spread1_min',\n",
       " 'spread1_max',\n",
       " 'spread1_std',\n",
       " 'spread1_q25',\n",
       " 'spread1_q75',\n",
       " 'spread1_mad',\n",
       " 'spread1_unique_pct',\n",
       " 'spread2_mean',\n",
       " 'spread2_median',\n",
       " 'spread2_max',\n",
       " 'spread2_std',\n",
       " 'spread2_q25',\n",
       " 'spread2_q75',\n",
       " 'spread2_mad',\n",
       " 'ask_spread_max',\n",
       " 'ask_spread_std',\n",
       " 'ask_spread_mad',\n",
       " 'bid_spread_max',\n",
       " 'bid_spread_std',\n",
       " 'bid_spread_mad',\n",
       " 'cross_spread1_mean',\n",
       " 'cross_spread1_median',\n",
       " 'cross_spread1_min',\n",
       " 'cross_spread1_max',\n",
       " 'cross_spread1_std',\n",
       " 'cross_spread1_q25',\n",
       " 'cross_spread1_q75',\n",
       " 'cross_spread1_mad',\n",
       " 'cross_spread1_unique_pct',\n",
       " 'cross_spread2_mean',\n",
       " 'cross_spread2_median',\n",
       " 'cross_spread2_max',\n",
       " 'cross_spread2_std',\n",
       " 'cross_spread2_q25',\n",
       " 'cross_spread2_q75',\n",
       " 'cross_spread2_mad',\n",
       " 'bas_mean',\n",
       " 'bas_median',\n",
       " 'bas_min',\n",
       " 'bas_max',\n",
       " 'bas_std',\n",
       " 'bas_q25',\n",
       " 'bas_q75',\n",
       " 'bas_mad',\n",
       " 'bas_unique_pct',\n",
       " 'trade_price_std',\n",
       " 'trade_price_mad',\n",
       " 'trade_price_delta_abs',\n",
       " 'trade_log_return_q25',\n",
       " 'trade_log_return_q75',\n",
       " 'trade_log_return_mad',\n",
       " 'target']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'trade_price','trade_log_return'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(train_df.drop('target',axis=1).columns)\n",
    "a.append('row_id')\n",
    "test_df = test_df[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_x , train_y = train_df.iloc[:,:-1],train_df.iloc[:,-1]\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(147,)))\n",
    "model.add(tf.keras.layers.Dense(10,activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(5,activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='tanh'))\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.MeanAbsolutePercentageError(),metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3351/3351 [==============================] - 4s 935us/step - loss: 145.2436 - root_mean_squared_error: 0.0154\n",
      "Epoch 2/10\n",
      "3351/3351 [==============================] - 3s 771us/step - loss: 28.1502 - root_mean_squared_error: 0.0016\n",
      "Epoch 3/10\n",
      "3351/3351 [==============================] - 3s 782us/step - loss: 26.6537 - root_mean_squared_error: 0.0016\n",
      "Epoch 4/10\n",
      "3351/3351 [==============================] - 3s 773us/step - loss: 25.0503 - root_mean_squared_error: 0.0015\n",
      "Epoch 5/10\n",
      "3351/3351 [==============================] - 3s 765us/step - loss: 24.1283 - root_mean_squared_error: 0.0015\n",
      "Epoch 6/10\n",
      "3351/3351 [==============================] - 3s 788us/step - loss: 23.2855 - root_mean_squared_error: 0.0015\n",
      "Epoch 7/10\n",
      "3351/3351 [==============================] - 3s 997us/step - loss: 22.9237 - root_mean_squared_error: 0.0015\n",
      "Epoch 8/10\n",
      "3351/3351 [==============================] - 4s 1ms/step - loss: 22.7336 - root_mean_squared_error: 0.0015\n",
      "Epoch 9/10\n",
      "3351/3351 [==============================] - 4s 1ms/step - loss: 22.7577 - root_mean_squared_error: 0.0015\n",
      "Epoch 10/10\n",
      "3351/3351 [==============================] - 5s 1ms/step - loss: 22.3314 - root_mean_squared_error: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19051f0cb48>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,train_y,batch_size=128,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_df.iloc[:,:-1])\n",
    "test_df['target'] = pred\n",
    "test_df[['row_id','target']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap1_mad</th>\n",
       "      <th>wap1_delta_abs</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wap2_mad</th>\n",
       "      <th>wap2_delta_abs</th>\n",
       "      <th>wap3_std</th>\n",
       "      <th>wap3_mad</th>\n",
       "      <th>wap3_delta_abs</th>\n",
       "      <th>wap4_std</th>\n",
       "      <th>...</th>\n",
       "      <th>bas_mad</th>\n",
       "      <th>bas_unique_pct</th>\n",
       "      <th>trade_price_std</th>\n",
       "      <th>trade_price_mad</th>\n",
       "      <th>trade_price_delta_abs</th>\n",
       "      <th>trade_log_return_q25</th>\n",
       "      <th>trade_log_return_q75</th>\n",
       "      <th>trade_log_return_mad</th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.00056</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.254967</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0-5</td>\n",
       "      <td>0.004273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.00099</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.271304</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>1-5</td>\n",
       "      <td>0.005091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   wap1_std  wap1_mad  wap1_delta_abs  wap2_std  wap2_mad  wap2_delta_abs  \\\n",
       "0  0.000693  0.000471        0.002298  0.000781   0.00056        0.002331   \n",
       "1  0.001174  0.000951        0.002346  0.001213   0.00099        0.002040   \n",
       "\n",
       "   wap3_std  wap3_mad  wap3_delta_abs  wap4_std  ...   bas_mad  \\\n",
       "0  0.000714  0.000481        0.002309  0.000698  ...  0.000154   \n",
       "1  0.001181  0.000958        0.002216  0.001181  ...  0.000137   \n",
       "\n",
       "   bas_unique_pct  trade_price_std  trade_price_mad  trade_price_delta_abs  \\\n",
       "0        0.254967         0.000578         0.000457               0.001345   \n",
       "1        0.271304         0.001034         0.000866               0.001036   \n",
       "\n",
       "   trade_log_return_q25  trade_log_return_q75  trade_log_return_mad  row_id  \\\n",
       "0             -0.000222              0.000295              0.000271     0-5   \n",
       "1             -0.000244              0.000326              0.000274     1-5   \n",
       "\n",
       "     target  \n",
       "0  0.004273  \n",
       "1  0.005091  \n",
       "\n",
       "[2 rows x 149 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
